{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we import python modules first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model, layers, Input\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "#from tensorflow.keras.optimizer_v2 import Nadam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Dense, Flatten, Reshape, Activation, InputLayer\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use followin function to import and normalize image. only three images are used as import. As a byproduct I am expecting to it can create some interesting images which may blend inputs together. imagedatagenerator is used here for the flexiblity, we may want to augment image later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions normalizes in the scale 0-1.\n",
    "# Since they are standarized before that, then dividing everything by 255 \n",
    "# won't work here\n",
    "def normalize(numpy_array):\n",
    "    minimum = numpy_array.min()\n",
    "    maximum = numpy_array.max()\n",
    "\n",
    "    normalized = (numpy_array - minimum)/(maximum - minimum)\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading 3 original pictures\n",
    "def loadImage(path, input_shape = INPUT_SHAPE):\n",
    "    img2 = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.resize(img2, (input_shape[1], input_shape[0]))\n",
    "    img2 = img2.reshape((1,) + img2.shape)\n",
    "    \n",
    "    mean = np.mean(img2)\n",
    "    std = np.std(img2)\n",
    "    data = (img2 - mean)/(std + 1e-7)\n",
    "    data = normalize(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainTensor():\n",
    "    path = '/Users/justin/projects/autoencoder/20210208_073907.jpg'\n",
    "    data = loadImage(path).copy()\n",
    "    \n",
    "    path = '/Users/justin/projects/autoencoder/2021-02-08_07-45-44.jpg'\n",
    "    data = np.append(data, loadImage(path).copy(), 0)\n",
    "    \n",
    "    path = '/Users/justin/projects/autoencoder/20210208_074628.jpg'\n",
    "    data = np.append(data, loadImage(path).copy(), 0)\n",
    "\n",
    "    train_gen = ImageDataGenerator(rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, zoom_range=0.0, fill_mode='nearest')\n",
    "\n",
    "    return train_gen.flow(data, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we only have 3 image input, we need to add a logic to make the image data generator to output arbitory batch size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_flow(batch_size = 12, mix_match = False):\n",
    "\n",
    "    input_data = loadTrainTensor()\n",
    "    while True:\n",
    "        input = None\n",
    "        for repeats, (train_data, _) in enumerate(input_data):\n",
    "            if repeats >= np.floor(batch_size/3): \n",
    "                break\n",
    "            if input is None:\n",
    "                input = train_data\n",
    "            else:\n",
    "                input = np.append(input, train_data.copy(), axis = 0)\n",
    "        \n",
    "        target = input.copy()\n",
    "        if mix_match:\n",
    "            np.random.shuffle(target)\n",
    "\n",
    "        yield input, target\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we keras class Model to create our encoder and decoder model classs. decoder and encoder have the reversed layer setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jencoder(Model):\n",
    "    def __init__(self, nlayers=7, filters=[32,64,128,196,256,512,1024], kernel_sizes=[3,3,3,3,3,3,3], strides = [2,2,2,2,2,2,2], image_shape=INPUT_SHAPE, latent_dim = LATENT_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        #check on parameters\n",
    "        assert(len(filters)==nlayers)\n",
    "        assert(len(kernel_sizes)==nlayers)\n",
    "        assert(len(strides)==nlayers)\n",
    "\n",
    "        #prepare layers and calculate last layer kernal size\n",
    "        self.nlayers = nlayers\n",
    "        self.conv_list = []\n",
    "        self.dense_list = []\n",
    "        self.batch_norm_list = []\n",
    "        size  = image_shape\n",
    "        activ = 'selu'\n",
    "\n",
    "        for i in range(nlayers):\n",
    "            self.conv_list.append(Conv2D(filters[i], kernel_sizes[i], activation=activ, strides = (strides[i], strides[i]), name = 'encode_conv_%d' % i))\n",
    "            self.batch_norm_list.append(BatchNormalization())\n",
    "            size = (int(tf.math.floor((size[0] - kernel_sizes[i])/strides[i]) + 1), int(tf.math.floor((size[1] - kernel_sizes[i])/strides[i]) + 1))\n",
    "            \n",
    "        self.last_conv_size = size\n",
    "\n",
    "        self.flatten_layer = Flatten()\n",
    "        self.dense_list.append(Dense(self.last_conv_size[0]*self.last_conv_size[1] * filters[-1] / 8 * 8, activation = activ, name = 'encode_dense_1'))\n",
    "        self.dense_list.append(Dense(self.last_conv_size[0]*self.last_conv_size[1] * filters[-1] / 8 * 4, activation = activ, name = 'encode_dense_2'))\n",
    "        self.dense_list.append(Dense(latent_dim, name = 'encode_dense_3'))\n",
    "    \n",
    "    def last_conv_size():\n",
    "        return self.last_conv_size\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        x = inputs\n",
    "        for i in range(self.nlayers):\n",
    "            x = self.conv_list[i](x)\n",
    "            x = self.batch_norm_list[i](x, training=training)\n",
    "\n",
    "        x = self.flatten_layer(x)\n",
    "        x = self.dense_list[0](x)\n",
    "        x = self.dense_list[1](x)\n",
    "        x = self.dense_list[2](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Jdecoder(Model):\n",
    "    def __init__(self, nlayers=8, filters=[1024, 512, 256, 196, 128, 64, 32, 3], kernel_sizes = [3,3,3,3,3,3,3,1], strides=[2,2,2,2,2,2,2,1], input_kernel_size = None):\n",
    "        super().__init__()\n",
    "\n",
    "        #check on parameters\n",
    "        assert(len(filters)==nlayers)\n",
    "        assert(len(kernel_sizes)==nlayers)\n",
    "        assert(len(strides)==nlayers)\n",
    "\n",
    "        #prepare layers and calculate last layer kernal size\n",
    "        self.nlayers = nlayers\n",
    "        self.conv_pos_list = []\n",
    "        self.batch_norm_list = []\n",
    "        activ = 'selu'\n",
    "        size  = input_kernel_size\n",
    "        for i in range(nlayers-1):\n",
    "            self.conv_pos_list.append(Conv2DTranspose(filters[i], kernel_sizes[i], activation=activ, strides = (strides[i], strides[i]), name = 'decode_conv_pos_%d' % i))\n",
    "            self.batch_norm_list.append(BatchNormalization())\n",
    "        \n",
    "        self.conv_pos_list.append(Conv2DTranspose(filters[nlayers-1], kernel_sizes[nlayers-1], activation='sigmoid', strides = (strides[nlayers-1], strides[nlayers-1]), name = 'decode_conv_pos_%d' % i))\n",
    "        self.batch_norm_list.append(BatchNormalization())\n",
    "\n",
    "        self.activation_layer = Activation(activ)\n",
    "        self.dense_layer = Dense(input_kernel_size[0]*input_kernel_size[1] * filters[0] / 8 * 8, activation = activ, name='decode_dense_1')\n",
    "        self.reshape_layer = Reshape((input_kernel_size[0], input_kernel_size[1], filters[0]))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.activation_layer(inputs)\n",
    "        x = self.dense_layer(x)\n",
    "        x = self.reshape_layer(x)\n",
    "\n",
    "        for i in range(self.nlayers):\n",
    "            x = self.batch_norm_list[i](x, training = training)            \n",
    "            x = self.conv_pos_list[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE model is composed by encoder and decoder. 2 additonal layers are added to estimate mean and variant of the distribution. Please take a look on isVaem switch. It makes this class model support both AEM and VAEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vaem(Model):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, image_shape = None, learning_rate = 0.00015, isVaem = False):\n",
    "        super().__init__()        \n",
    "        \n",
    "        self.isVaem = isVaem\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_shape = image_shape\n",
    " \n",
    "        self.z_mean_layer = Dense(latent_dim, name='z_mean_layer')\n",
    "        self.z_var_layer = Dense(latent_dim, name='z_var_layer')\n",
    "\n",
    "        self.optimizer = Nadam(learning_rate = learning_rate)\n",
    "        self.batch_norm_z_mean = BatchNormalization(name = 'batch_norm_z_mean')\n",
    "        self.batch_norm_z_log_var = BatchNormalization(name='batch_norm_z_log_var')\n",
    "\n",
    "        loss_tracker = metrics.Mean(name=\"loss\")\n",
    "        mae_metric = metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "    def aem_loss_func(self, input, decoded):\n",
    "        return MeanSquaredError()(input, decoded)\n",
    "\n",
    "    def vaem_loss_func(self, input, decoded, z_mean, z_log_var):\n",
    "        x = K.flatten(input)\n",
    "        x_decoded = K.flatten(decoded)\n",
    "\n",
    "        #xent_loss = tf.keras.metrics.binary_crossentropy(x, x_decoded)\n",
    "        xent_loss = MeanSquaredError()(x, x_decoded)\n",
    "\n",
    "        kl_loss = -5e-4*K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        loss = K.mean(xent_loss + kl_loss)    \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def z_sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latent_dim), mean=0., stddev=1.)\n",
    "        zi = z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "        return zi\n",
    "\n",
    "    def compile(self):\n",
    "        super().compile()\n",
    "\n",
    "        self.encoder = Jencoder(image_shape = self.image_shape, latent_dim = self.latent_dim)\n",
    "        self.last_conv_size = self.encoder.last_conv_size\n",
    "        self.encoder.build((None,) + INPUT_SHAPE)\n",
    "        \n",
    "        if self.isVaem:\n",
    "            self.z_mean_layer.build((None, self.latent_dim))\n",
    "            self.z_var_layer.build((None, self.latent_dim))\n",
    "\n",
    "        self.decoder = Jdecoder(input_kernel_size = self.last_conv_size)\n",
    "        self.decoder.build((None, self.latent_dim))\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.mae_metric = tf.keras.metrics.MeanAbsoluteError(name=\"diff\")\n",
    "\n",
    "        # set train-step and call logic based on isVaem switch\n",
    "        self.train_step = self.VAEM_step if self.isVaem else self.AEM_step\n",
    "        self.call = self.VAEM_call if self.isVaem else self.AEM_call\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    def VAEM_call(self, input, training = False):\n",
    "        x = self.encoder(input, training = training)\n",
    "\n",
    "        z_mean = self.z_mean_layer(x)\n",
    "        z_mean = self.batch_norm_z_mean(z_mean, training= training)\n",
    "\n",
    "        z_log_var = self.z_var_layer(x)\n",
    "        z_log_var = self.batch_norm_z_log_var(z_log_var, training = training)\n",
    "\n",
    "        z_sample = self.z_sampling((z_mean, z_log_var))\n",
    "        x = self.decoder(z_sample, training= training)\n",
    "        return x       \n",
    "\n",
    "    def AEM_call(self, input, training = False):\n",
    "        z_sample = self.encoder(input, training= training)\n",
    "        x = self.decoder(z_sample, training= training)\n",
    "        return x    \n",
    "\n",
    "    def VAEM_step(self, data):\n",
    "        inp, tar = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = self.encoder(inp, training=True)\n",
    "            \n",
    "            z_mean = self.z_mean_layer(x)\n",
    "            z_mean = self.batch_norm_z_mean(z_mean, training= True)\n",
    "            z_log_var = self.z_var_layer(x)\n",
    "            z_log_var = self.batch_norm_z_log_var(z_log_var, training = True)\n",
    "\n",
    "            z = self.z_sampling((z_mean, z_log_var))\n",
    "            target = self.decoder(z, training = True)\n",
    "\n",
    "            loss = self.vaem_loss_func(tar, target, z_mean, z_log_var)            \n",
    "            trainable_vars = self.trainable_variables\n",
    "            #self.encoder.trainable_variables + self.z_mean_layer.trainable_variables + self.z_var_layer.trainable_variables + self.decoder.trainable_variables\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mae_metric.update_state(tar, target)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"diff\": self.mae_metric.result()}\n",
    "\n",
    "    def AEM_step(self, data):\n",
    "        inp, tar = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.encoder(inp, training = True)\n",
    "            target = self.decoder(z, training = True)\n",
    "\n",
    "            loss = self.aem_loss_func(inp, target)\n",
    "            trainable_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "    \n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mae_metric.update_state(tar, target)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"diff\": self.mae_metric.result()}\n",
    "\n",
    "    # list loss_tracker and mae_metric here so that they will be reset for each epoch in training\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.mae_metric]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    logdir = 'jmodel' + datetime.now().strftime('%m_%d_%YT%H')\n",
    "    tbpath = '/Users/justin/projects/autoencoder/tensorboard/%s' % logdir\n",
    "    filepathstr = 'weights_%s.tf' % logdir\n",
    "    tensor_board_callback = TensorBoard(\n",
    "        log_dir = tbpath,\n",
    "        histogram_freq=1,\n",
    "        update_freq = 'epoch',\n",
    "        write_graph = True,\n",
    "        embeddings_freq = 0)\n",
    "    reduce_lr_callback = ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.618,\n",
    "        patience=25,\n",
    "        mode = 'auto',\n",
    "        verbose=1,\n",
    "        min_delta = 0.0001)\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=filepathstr,\n",
    "        save_weights_only=True,\n",
    "        monitor='loss',\n",
    "        mode='min',\n",
    "        verbose = 1,\n",
    "        save_best_only=True)\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='loss', \n",
    "        patience = 100,\n",
    "        restore_best_weights=True)\n",
    "\n",
    "    hist = vae.fit(custom_data_flow(BATCH_SIZE), batch_size = BATCH_SIZE, shuffle = False, epochs = EPOCHS, steps_per_epoch = STEPS_IN_EPOCH,\n",
    "        callbacks=[reduce_lr_callback, model_checkpoint_callback, early_stopping_callback, tensor_board_callback])\n",
    "\n",
    "    loss = min(hist.history['loss'])\n",
    "    df = pd.DataFrame(hist.history['loss'])\n",
    "    df.to_csv('weights_%s.tf.loss%.4f' % (logdir, loss))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "INPUT_SHAPE = (255, 255, 3)\n",
    "LATENT_DIM = 4\n",
    "    \n",
    "EPOCHS = 1000\n",
    "STEPS_IN_EPOCH = 20\n",
    "BATCH_SIZE = 12 \n",
    "\n",
    "vae = Vaem(latent_dim = LATENT_DIM, image_shape = INPUT_SHAPE, isVaem = True, learning_rate = 0.0000618)\n",
    "vae.compile()\n",
    "\n",
    "# use this option for debugging only\n",
    "# vae.run_eagerly = True\n",
    "\n",
    "vae.build((None,) + INPUT_SHAPE)\n",
    "vae.summary()\n",
    "train_model(vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env - tf24",
   "language": "python",
   "name": "tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
